#!/usr/bin/env bash
# Return on any failure
set -e

# TODO: make sure travis has the right version of nose
if [[ $# -gt 0 ]]; then
    noseOptionsArr=()
    for tf in ${@}; do noseOptionsArr+=("tests/$tf"); done
else
    # add all python files in the test dir recursively
    noseOptionsArr=($(find tests -type f | grep -E "\.py$" | grep -v "__init__.py"))
fi

# Limit TensorFlow error message
# https://github.com/tensorflow/tensorflow/issues/1258
export TF_CPP_MIN_LOG_LEVEL=3

for noseOptions in ${noseOptionsArr[@]}; do
  echo "============= Running the tests in: $noseOptions ============="

  # The grep below is a horrible hack for spark 1.x:
  # => we manually remove some log lines to stay below the 4MB log limit on Travis.
  $PYSPARK_DRIVER_PYTHON \
      -m "nose" \
      --nocapture --nologcapture \
      -v --exe "$noseOptions" \
      2>&1 | grep -vE "INFO (ParquetOutputFormat|SparkContext|ContextCleaner|ShuffleBlockFetcherIterator|MapOutputTrackerMaster|TaskSetManager|Executor|MemoryStore|CacheManager|BlockManager|DAGScheduler|PythonRDD|TaskSchedulerImpl|ZippedPartitionsRDD2)";

  # Exit immediately if the tests fail.
  # Since we pipe to remove the output, we need to use some horrible BASH features:
  # http://stackoverflow.com/questions/1221833/bash-pipe-output-and-capture-exit-status
  test ${PIPESTATUS[0]} -eq 0 || exit 1;
done


# Run doc tests

#$PYSPARK_PYTHON -u ourpythonfilewheneverwehaveone.py "$@"
